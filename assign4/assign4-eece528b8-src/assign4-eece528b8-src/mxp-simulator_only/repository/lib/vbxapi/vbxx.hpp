/* VECTORBLOX MXP SOFTWARE DEVELOPMENT KIT
 *
 * Copyright (C) 2012-2017 VectorBlox Computing Inc., Vancouver, British Columbia, Canada.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 *     * Redistributions of source code must retain the above copyright
 *       notice, this list of conditions and the following disclaimer.
 *
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *
 *     * Neither the name of VectorBlox Computing Inc. nor the
 *       names of its contributors may be used to endorse or promote products
 *       derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * This agreement shall be governed in all respects by the laws of the Province
 * of British Columbia and by the laws of Canada.
 *
 * This file is part of the VectorBlox MXP Software Development Kit.
 *
 */

#ifndef __VBXX_HPP
#define __VBXX_HPP

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVBHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVBHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVBHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVBHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVBHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVBHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVBHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVBHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVBHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVBHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVBHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVBHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVBHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVBHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVBHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVBHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVBHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVBHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVBHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVBHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVBHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVBHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVBHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVBHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVBHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVBHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVBHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVBHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVBHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVBHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVBHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVBHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVBHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVBHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVBHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVBHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVBHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVBHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVBHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVBHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVBHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVBWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVBWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVBWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVBWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVBWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVBWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVBWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVBWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVBWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVBWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVBWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVBWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVBWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVBWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVBWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVBWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVBWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVBWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVBWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVBWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVBWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVBWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVBWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVBWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVBWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVBWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVBWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVBWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVBWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVBWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVBWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVBWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVBWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVBWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVBWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVBWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVBWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVBWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVBWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVBWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVBWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVHBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVHBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVHBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVHBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVHBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVHBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVHBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVHBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVHBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVHBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVHBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVHBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVHBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVHBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVHBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVHBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVHBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVHBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVHBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVHBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVHBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVHBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVHBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVHBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVHBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVHBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVHBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVHBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVHBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVHBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVHBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVHBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVHBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVHBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVHBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVHBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVHBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVHBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVHBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVHBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVHBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVHWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVHWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVHWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVHWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVHWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVHWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVHWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVHWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVHWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVHWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVHWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVHWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVHWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVHWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVHWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVHWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVHWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVHWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVHWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVHWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVHWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVHWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVHWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVHWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVHWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVHWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVHWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVHWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVHWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVHWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVHWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVHWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVHWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVHWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVHWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVHWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVHWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVHWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVHWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVHWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVHWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVWBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVWBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVWBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVWBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVWBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVWBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVWBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVWBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVWBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVWBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVWBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVWBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVWBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVWBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVWBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVWBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVWBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVWBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVWBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVWBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVWBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVWBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVWBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVWBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVWBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVWBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVWBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVWBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVWBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVWBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVWBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVWBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVWBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVWBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVWBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVWBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVWBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVWBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVWBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVWBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVWBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVWHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVWHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVWHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVWHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVWHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVWHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVWHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVWHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVWHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVWHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVWHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVWHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVWHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVWHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVWHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVWHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVWHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVWHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVWHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVWHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVWHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVWHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVWHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVWHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVWHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVWHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVWHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVWHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVWHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVWHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVWHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVWHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVWHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVWHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVWHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVWHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVWHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVWHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVWHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVWHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVWHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( VVWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( VVWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( SVBS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVBHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVBHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVBHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVBHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVBHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVBHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVBHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVBHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVBHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVBHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVBHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVBHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVBHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVBHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVBHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVBHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVBHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVBHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVBHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVBHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVBHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVBHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVBHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVBHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVBHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVBHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVBHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVBHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVBHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVBHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVBHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVBHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVBHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVBHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVBHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVBHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVBHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVBHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVBHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVBHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVBWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVBWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVBWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVBWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVBWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVBWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVBWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVBWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVBWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVBWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVBWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVBWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVBWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVBWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVBWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVBWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVBWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVBWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVBWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVBWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVBWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVBWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVBWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVBWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVBWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVBWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVBWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVBWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVBWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVBWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVBWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVBWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVBWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVBWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVBWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVBWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVBWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVBWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVBWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVBWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVHBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVHBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVHBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVHBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVHBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVHBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVHBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVHBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVHBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVHBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVHBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVHBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVHBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVHBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVHBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVHBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVHBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVHBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVHBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVHBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVHBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVHBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVHBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVHBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVHBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVHBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVHBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVHBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVHBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVHBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVHBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVHBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVHBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVHBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVHBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVHBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVHBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVHBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVHBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVHBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( SVHS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVHWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVHWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVHWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVHWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVHWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVHWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVHWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVHWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVHWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVHWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVHWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVHWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVHWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVHWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVHWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVHWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVHWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVHWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVHWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVHWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVHWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVHWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVHWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVHWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVHWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVHWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVHWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVHWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVHWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVHWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVHWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVHWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVHWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVHWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVHWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVHWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVHWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVHWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVHWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVHWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVWBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVWBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVWBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVWBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVWBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVWBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVWBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVWBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVWBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVWBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVWBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVWBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVWBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVWBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVWBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVWBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVWBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVWBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVWBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVWBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVWBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVWBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVWBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVWBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVWBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVWBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVWBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVWBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVWBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVWBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVWBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVWBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVWBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVWBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVWBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVWBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVWBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVWBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVWBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVWBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVWHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVWHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVWHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVWHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVWHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVWHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVWHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVWHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVWHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVWHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVWHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVWHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVWHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVWHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVWHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVWHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVWHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVWHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVWHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVWHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVWHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVWHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVWHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVWHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVWHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVWHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVWHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVWHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVWHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVWHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVWHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVWHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVWHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVWHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVWHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVWHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVWHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVWHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVWHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVWHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( SVWS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm( SVWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm( SVWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEBHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEBHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEBHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEBHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEBHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEBHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEBHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEBHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEBHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEBHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEBHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEBHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEBHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEBHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEBHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEBHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEBWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEBWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEBWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEBWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEBWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEBWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEBWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEBWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEBWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEBWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEBWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEBWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEBWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEBWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEBWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEBWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEHBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEHBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEHBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEHBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEHBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEHBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEHBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEHBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEHBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEHBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEHBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEHBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEHBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEHBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEHBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEHBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEHWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEHWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEHWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEHWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEHWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEHWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEHWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEHWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEHWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEHWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEHWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEHWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEHWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEHWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEHWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEHWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEWBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEWBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEWBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEWBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEWBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEWBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEWBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEWBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEWBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEWBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEWBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEWBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEWBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEWBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEWBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEWBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEWHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEWHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEWHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEWHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEWHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEWHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEWHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEWHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEWHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEWHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEWHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEWHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEWHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEWHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEWHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEWHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( VEWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( VEWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SEBS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEBS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( SEBS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( SEBS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEBS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEBS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEBS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEBS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEBS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEBS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEBS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEBS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEBS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEBS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEBS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEBS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEBS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEBS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEBS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEBS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEBS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEBS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEBS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEBS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEBS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEBS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEBS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEBS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEBS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEBS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEBS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEBS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEBS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEBS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEBS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEBS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEBS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEBS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEBS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( SEBU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( SEBU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SEHS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEHS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( SEHS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( SEHS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEHS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEHS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEHS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEHS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEHS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEHS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEHS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEHS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEHS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEHS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEHS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEHS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEHS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEHS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEHS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEHS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEHS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEHS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEHS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEHS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEHS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEHS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEHS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEHS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEHS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEHS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEHS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEHS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEHS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEHS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEHS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEHS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEHS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEHS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEHS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( SEHU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( SEHU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SEWS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEWS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( SEWS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( SEWS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEWS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEWS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEWS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEWS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEWS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEWS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEWS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEWS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEWS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEWS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEWS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEWS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEWS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEWS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEWS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEWS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEWS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEWS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEWS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEWS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEWS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEWS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEWS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEWS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEWS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEWS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEWS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEWS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEWS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEWS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEWS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEWS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEWS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEWS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEWS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm( SEWU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm( SEWU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVBHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVBHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVBHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVBHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVBHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVBHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVBHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVBHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVBHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVBHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVBHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVBHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVBHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVBHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVBHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVBHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVBHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVBHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVBHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVBHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVBHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVBHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVBHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVBWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVBWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVBWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVBWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVBWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVBWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVBWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVBWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVBWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVBWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVBWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVBWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVBWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVBWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVBWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVBWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVBWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVBWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVBWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVBWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVBWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVBWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVBWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVHBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVHBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVHBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVHBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVHBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVHBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVHBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVHBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVHBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVHBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVHBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVHBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVHBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVHBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVHBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVHBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVHBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVHBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVHBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVHBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVHBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVHBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVHBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVHWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVHWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVHWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVHWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVHWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVHWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVHWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVHWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVHWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVHWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVHWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVHWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVHWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVHWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVHWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVHWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVHWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVHWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVHWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVHWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVHWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVHWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVHWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVWBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVWBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVWBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVWBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVWBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVWBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVWBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVWBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVWBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVWBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVWBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVWBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVWBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVWBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVWBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVWBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVWBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVWBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVWBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVWBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVWBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVWBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVWBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVWHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVWHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVWHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVWHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVWHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVWHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVWHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVWHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVWHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVWHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVWHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVWHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVWHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVWHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVWHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVWHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVWHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVWHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVWHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVWHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVWHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVWHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVWHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( VVWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( VVWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( SVBS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVBHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVBHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVBHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVBHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVBHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVBHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVBHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVBHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVBHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVBHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVBHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVBHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVBHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVBHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVBHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVBHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVBHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVBHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVBHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVBHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVBHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVBHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVBHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVBWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVBWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVBWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVBWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVBWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVBWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVBWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVBWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVBWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVBWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVBWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVBWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVBWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVBWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVBWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVBWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVBWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVBWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVBWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVBWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVBWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVBWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVBWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVHBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVHBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVHBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVHBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVHBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVHBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVHBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVHBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVHBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVHBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVHBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVHBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVHBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVHBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVHBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVHBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVHBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVHBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVHBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVHBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVHBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVHBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVHBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( SVHS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVHWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVHWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVHWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVHWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVHWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVHWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVHWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVHWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVHWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVHWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVHWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVHWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVHWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVHWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVHWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVHWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVHWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVHWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVHWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVHWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVHWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVHWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVHWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVWBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVWBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVWBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVWBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVWBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVWBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVWBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVWBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVWBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVWBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVWBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVWBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVWBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVWBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVWBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVWBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVWBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVWBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVWBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVWBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVWBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVWBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVWBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVWHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVWHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVWHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVWHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVWHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVWHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVWHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVWHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVWHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVWHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVWHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVWHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVWHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVWHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVWHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVWHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVWHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVWHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVWHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVWHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVWHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVWHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVWHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( SVWS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked( SVWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked( SVWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEBHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEBHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEBHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEBHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEBWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEBWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEBWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEBWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEHBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEHBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEHBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEHBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEHWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEHWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEHWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEHWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEWBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEWBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEWBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEWBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEWHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEWHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEWHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEWHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( VEWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( VEWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEBS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEBS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( SEBS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( SEBS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEBS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEBS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEBS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEBS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEBS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEBS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEBS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEBS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEBS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEBS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEBS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEBS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEBS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEBS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEBS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEBS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEBS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEBS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEBS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEBS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEBS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEBS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEBS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEBS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEBS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEBS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEBS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEBS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEBS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEBS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEBS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEBS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEBS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEBS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEBS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( SEBU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( SEBU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEHS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEHS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( SEHS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( SEHS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEHS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEHS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEHS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEHS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEHS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEHS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEHS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEHS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEHS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEHS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEHS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEHS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEHS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEHS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEHS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEHS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEHS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEHS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEHS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEHS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEHS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEHS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEHS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEHS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEHS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEHS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEHS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEHS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEHS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEHS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEHS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEHS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEHS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEHS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEHS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( SEHU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( SEHU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEWS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEWS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( SEWS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( SEWS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEWS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEWS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEWS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEWS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEWS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEWS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEWS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEWS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEWS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEWS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEWS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEWS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEWS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEWS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEWS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEWS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEWS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEWS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEWS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEWS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEWS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEWS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEWS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEWS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEWS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEWS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEWS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEWS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEWS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEWS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEWS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEWS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEWS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEWS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEWS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked( SEWU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked( SEWU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVBHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVBHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVBHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVBHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVBHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVBHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVBHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVBHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVBHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVBHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVBHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVBHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVBHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVBHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVBHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVBHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVBHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVBHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVBHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVBHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVBHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVBHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVBHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVBWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVBWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVBWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVBWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVBWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVBWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVBWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVBWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVBWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVBWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVBWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVBWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVBWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVBWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVBWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVBWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVBWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVBWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVBWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVBWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVBWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVBWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVBWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVHBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVHBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVHBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVHBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVHBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVHBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVHBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVHBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVHBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVHBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVHBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVHBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVHBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVHBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVHBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVHBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVHBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVHBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVHBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVHBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVHBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVHBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVHBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVHWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVHWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVHWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVHWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVHWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVHWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVHWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVHWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVHWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVHWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVHWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVHWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVHWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVHWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVHWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVHWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVHWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVHWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVHWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVHWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVHWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVHWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVHWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVWBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVWBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVWBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVWBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVWBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVWBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVWBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVWBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVWBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVWBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVWBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVWBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVWBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVWBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVWBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVWBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVWBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVWBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVWBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVWBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVWBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVWBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVWBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVWHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVWHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVWHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVWHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVWHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVWHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVWHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVWHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVWHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVWHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVWHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVWHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVWHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVWHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVWHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVWHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVWHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVWHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVWHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVWHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVWHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVWHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVWHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( VVWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( VVWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( SVBS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVBHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVBHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVBHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVBHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVBHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVBHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVBHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVBHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVBHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVBHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVBHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVBHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVBHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVBHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVBHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVBHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVBHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVBHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVBHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVBHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVBHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVBHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVBHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVBWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVBWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVBWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVBWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVBWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVBWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVBWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVBWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVBWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVBWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVBWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVBWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVBWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVBWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVBWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVBWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVBWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVBWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVBWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVBWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVBWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVBWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVBWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVHBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVHBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVHBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVHBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVHBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVHBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVHBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVHBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVHBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVHBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVHBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVHBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVHBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVHBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVHBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVHBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVHBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVHBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVHBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVHBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVHBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVHBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVHBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( SVHS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVHWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVHWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVHWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVHWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVHWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVHWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVHWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVHWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVHWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVHWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVHWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVHWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVHWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVHWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVHWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVHWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVHWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVHWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVHWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVHWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVHWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVHWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVHWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVWBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVWBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVWBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVWBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVWBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVWBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVWBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVWBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVWBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVWBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVWBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVWBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVWBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVWBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVWBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVWBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVWBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVWBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVWBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVWBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVWBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVWBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVWBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVWHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVWHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVWHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVWHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVWHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVWHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVWHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVWHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVWHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVWHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVWHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVWHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVWHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVWHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVWHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVWHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVWHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVWHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVWHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVWHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVWHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVWHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVWHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( SVWS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc( SVWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc( SVWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEBHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEBHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEBHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEBHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEBWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEBWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEBWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEBWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEHBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEHBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEHBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEHBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEHWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEHWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEHWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEHWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEWBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEWBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEWBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEWBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEWHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEWHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEWHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEWHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( VEWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( VEWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEBS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEBS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( SEBS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( SEBS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEBS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEBS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEBS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEBS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEBS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEBS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEBS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEBS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEBS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEBS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEBS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEBS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEBS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEBS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEBS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEBS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEBS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEBS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEBS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEBS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEBS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEBS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEBS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEBS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEBS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEBS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEBS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEBS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEBS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEBS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEBS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEBS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEBS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEBS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEBS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( SEBU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( SEBU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEHS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEHS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( SEHS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( SEHS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEHS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEHS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEHS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEHS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEHS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEHS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEHS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEHS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEHS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEHS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEHS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEHS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEHS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEHS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEHS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEHS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEHS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEHS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEHS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEHS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEHS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEHS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEHS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEHS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEHS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEHS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEHS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEHS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEHS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEHS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEHS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEHS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEHS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEHS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEHS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( SEHU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( SEHU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEWS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEWS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( SEWS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( SEWS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEWS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEWS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEWS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEWS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEWS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEWS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEWS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEWS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEWS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEWS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEWS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEWS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEWS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEWS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEWS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEWS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEWS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEWS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEWS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEWS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEWS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEWS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEWS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEWS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEWS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEWS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEWS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEWS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEWS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEWS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEWS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEWS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEWS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEWS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEWS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc( SEWU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc( SEWU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVBHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVBHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVBHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVBHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVBHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVBHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVBHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVBHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVBHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVBHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVBHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVBWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVBWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVBWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVBWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVBWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVBWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVBWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVBWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVBWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVBWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVBWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVHBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVHBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVHBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVHBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVHBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVHBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVHBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVHBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVHBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVHBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVHBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVHWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVHWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVHWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVHWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVHWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVHWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVHWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVHWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVHWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVHWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVHWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVWBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVWBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVWBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVWBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVWBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVWBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVWBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVWBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVWBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVWBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVWBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVWHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVWHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVWHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVWHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVWHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVWHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVWHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVWHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVWHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVWHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVWHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VVWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VVWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( SVBS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVBHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVBHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVBHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVBHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVBHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVBHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVBHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVBHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVBHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVBHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVBHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVBHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVBWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVBWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVBWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVBWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVBWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVBWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVBWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVBWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVBWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVBWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVBWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVBWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVHBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVHBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVHBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVHBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVHBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVHBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVHBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVHBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVHBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVHBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVHBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVHBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( SVHS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVHWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVHWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVHWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVHWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVHWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVHWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVHWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVHWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVHWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVHWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVHWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVHWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVWBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVWBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVWBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVWBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVWBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVWBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVWBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVWBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVWBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVWBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVWBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVWBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVWHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVWHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVWHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVWHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVWHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVWHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVWHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVWHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVWHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVWHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVWHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVWHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( SVWS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_masked_acc( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SVWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SVWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_masked_acc( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_masked_acc( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_masked_acc( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_masked_acc( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_masked_acc( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_masked_acc( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_masked_acc( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_masked_acc( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_masked_acc( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_masked_acc( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEBHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEBHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEBHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEBHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEBWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEBWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEBWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEBWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEHBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEHBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEHBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEHBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEHWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEHWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEHWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEHWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEWBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEWBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEWBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEWBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEWHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEWHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEWHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEWHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( VEWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( VEWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEBS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEBS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SEBS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SEBS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEBS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEBS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEBS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEBS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEBS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEBS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEBS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEBS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEBS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEBS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEBS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEBS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEBS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEBS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEBS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEBS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEBS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEBS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEBS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEBS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEBS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEBS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEBS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEBS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEBS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEBS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEBS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEBS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEBS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEBS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEBS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEBS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEBS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEBS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEBS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SEBU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SEBU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEHS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEHS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SEHS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SEHS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEHS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEHS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEHS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEHS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEHS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEHS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEHS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEHS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEHS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEHS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEHS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEHS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEHS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEHS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEHS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEHS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEHS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEHS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEHS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEHS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEHS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEHS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEHS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEHS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEHS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEHS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEHS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEHS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEHS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEHS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEHS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEHS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEHS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEHS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEHS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SEHU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SEHU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEWS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEWS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SEWS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SEWS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEWS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEWS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEWS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEWS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEWS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEWS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEWS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEWS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEWS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEWS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEWS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEWS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEWS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEWS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEWS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEWS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEWS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEWS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEWS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEWS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEWS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEWS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEWS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEWS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEWS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEWS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEWS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEWS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEWS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEWS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEWS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEWS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEWS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEWS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEWS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_masked_acc( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_masked_acc( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_masked_acc( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_masked_acc( SEWU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_masked_acc( SEWU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_masked_acc( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_masked_acc( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_masked_acc( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_masked_acc( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_masked_acc( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_masked_acc( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_masked_acc( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_masked_acc( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_masked_acc( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_masked_acc( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_masked_acc( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_masked_acc( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_masked_acc( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_masked_acc( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_masked_acc( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_masked_acc( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_masked_acc( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_masked_acc( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_masked_acc( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_masked_acc( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_masked_acc( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_masked_acc( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_masked_acc( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_masked_acc( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_masked_acc( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_masked_acc( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_masked_acc( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_masked_acc( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_masked_acc( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_masked_acc( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_masked_acc( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_masked_acc( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_masked_acc( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_masked_acc( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_masked_acc( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVBHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVBHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVBHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVBHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVBHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVBHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVBHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVBHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVBHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVBHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVBHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVBHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVBHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVBHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVBHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVBHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVBHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVBHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVBHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVBHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVBHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVBHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVBHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVBWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVBWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVBWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVBWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVBWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVBWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVBWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVBWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVBWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVBWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVBWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVBWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVBWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVBWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVBWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVBWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVBWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVBWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVBWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVBWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVBWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVBWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVBWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVHBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVHBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVHBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVHBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVHBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVHBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVHBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVHBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVHBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVHBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVHBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVHBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVHBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVHBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVHBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVHBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVHBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVHBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVHBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVHBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVHBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVHBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVHBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVHWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVHWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVHWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVHWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVHWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVHWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVHWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVHWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVHWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVHWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVHWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVHWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVHWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVHWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVHWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVHWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVHWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVHWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVHWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVHWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVHWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVHWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVHWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVWBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVWBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVWBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVWBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVWBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVWBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVWBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVWBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVWBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVWBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVWBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVWBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVWBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVWBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVWBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVWBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVWBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVWBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVWBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVWBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVWBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVWBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVWBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVWHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVWHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVWHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVWHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVWHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVWHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVWHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVWHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVWHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVWHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVWHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVWHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVWHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVWHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVWHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVWHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVWHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVWHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVWHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVWHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVWHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVWHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVWHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( VVWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( VVWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( SVBS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVBHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVBHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVBHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVBHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVBHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVBHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVBHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVBHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVBHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVBHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVBHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVBHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVBHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVBHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVBHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVBHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVBHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVBHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVBHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVBHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVBHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVBHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVBHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVBWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVBWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVBWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVBWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVBWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVBWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVBWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVBWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVBWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVBWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVBWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVBWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVBWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVBWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVBWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVBWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVBWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVBWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVBWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVBWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVBWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVBWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVBWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVHBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVHBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVHBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVHBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVHBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVHBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVHBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVHBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVHBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVHBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVHBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVHBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVHBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVHBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVHBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVHBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVHBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVHBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVHBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVHBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVHBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVHBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVHBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( SVHS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVHWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVHWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVHWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVHWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVHWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVHWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVHWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVHWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVHWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVHWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVHWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVHWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVHWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVHWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVHWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVHWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVHWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVHWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVHWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVHWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVHWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVHWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVHWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVWBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVWBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVWBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVWBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVWBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVWBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVWBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVWBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVWBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVWBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVWBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVWBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVWBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVWBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVWBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVWBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVWBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVWBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVWBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVWBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVWBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVWBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVWBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVWHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVWHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVWHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVWHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVWHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVWHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVWHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVWHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVWHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVWHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVWHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVWHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVWHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVWHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVWHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVWHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVWHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVWHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVWHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVWHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVWHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVWHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVWHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( SVWS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_2D( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_2D( SVWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_2D( SVWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_2D( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_2D( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_2D( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_2D( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_2D( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_2D( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_2D( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_2D( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_2D( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_2D( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_2D( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_2D( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_2D( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_2D( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_2D( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEBHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEBHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEBHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEBHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEBWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEBWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEBWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEBWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEHBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEHBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEHBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEHBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEHWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEHWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEHWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEHWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEWBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEWBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEWBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEWBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEWHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEWHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEWHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEWHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( VEWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( VEWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEBS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEBS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( SEBS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( SEBS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEBS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEBS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEBS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEBS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEBS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEBS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEBS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEBS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEBS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEBS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEBS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEBS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEBS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEBS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEBS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEBS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEBS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEBS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEBS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEBS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEBS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEBS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEBS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEBS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEBS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEBS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEBS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEBS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEBS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEBS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEBS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEBS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEBS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEBS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEBS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( SEBU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( SEBU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEHS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEHS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( SEHS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( SEHS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEHS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEHS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEHS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEHS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEHS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEHS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEHS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEHS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEHS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEHS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEHS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEHS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEHS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEHS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEHS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEHS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEHS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEHS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEHS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEHS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEHS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEHS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEHS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEHS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEHS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEHS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEHS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEHS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEHS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEHS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEHS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEHS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEHS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEHS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEHS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( SEHU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( SEHU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEWS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEWS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( SEWS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( SEWS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEWS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEWS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEWS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEWS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEWS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEWS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEWS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEWS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEWS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEWS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEWS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEWS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEWS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEWS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEWS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEWS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEWS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEWS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEWS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEWS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEWS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEWS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEWS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEWS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEWS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEWS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEWS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEWS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEWS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEWS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEWS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEWS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEWS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEWS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEWS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_2D( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_2D( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_2D( SEWU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_2D( SEWU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_2D( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_2D( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_2D( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_2D( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_2D( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_2D( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_2D( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_2D( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_2D( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_2D( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_2D( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_2D( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_2D( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_2D( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_2D( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_2D( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_2D( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_2D( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_2D( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_2D( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_2D( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_2D( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_2D( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_2D( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_2D( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_2D( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_2D( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_2D( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_2D( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_2D( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_2D( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_2D( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_2D( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_2D( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_2D( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVBHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVBHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVBHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVBHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVBHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVBHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVBHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVBHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVBHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVBHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVBHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVBWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVBWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVBWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVBWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVBWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVBWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVBWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVBWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVBWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVBWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVBWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVHBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVHBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVHBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVHBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVHBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVHBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVHBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVHBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVHBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVHBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVHBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVHWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVHWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVHWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVHWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVHWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVHWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVHWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVHWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVHWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVHWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVHWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVWBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVWBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVWBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVWBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVWBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVWBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVWBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVWBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVWBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVWBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVWBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVWHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVWHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVWHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVWHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVWHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVWHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVWHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVWHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVWHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVWHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVWHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VVWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VVWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( SVBS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVBHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVBHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVBHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVBHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVBHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVBHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVBHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVBHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVBHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVBHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVBHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVBHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVBWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVBWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVBWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVBWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVBWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVBWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVBWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVBWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVBWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVBWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVBWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVBWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVHBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVHBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVHBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVHBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVHBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVHBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVHBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVHBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVHBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVHBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVHBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVHBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( SVHS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVHWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVHWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVHWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVHWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVHWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVHWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVHWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVHWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVHWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVHWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVHWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVHWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVWBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVWBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVWBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVWBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVWBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVWBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVWBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVWBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVWBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVWBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVWBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVWBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVWHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVWHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVWHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVWHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVWHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVWHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVWHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVWHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVWHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVWHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVWHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVWHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( SVWS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_2D( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SVWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SVWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_2D( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_2D( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_2D( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_2D( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_2D( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_2D( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_2D( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_2D( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_2D( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_2D( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEBHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEBHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEBHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEBHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEBWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEBWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEBWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEBWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEHBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEHBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEHBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEHBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEHWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEHWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEHWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEHWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEWBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEWBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEWBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEWBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEWHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEWHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEWHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEWHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( VEWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( VEWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEBS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEBS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SEBS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SEBS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEBS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEBS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEBS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEBS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEBS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEBS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEBS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEBS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEBS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEBS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEBS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEBS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEBS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEBS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEBS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEBS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEBS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEBS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEBS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEBS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEBS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEBS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEBS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEBS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEBS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEBS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEBS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEBS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEBS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEBS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEBS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEBS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEBS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEBS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEBS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SEBU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SEBU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEHS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEHS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SEHS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SEHS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEHS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEHS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEHS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEHS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEHS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEHS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEHS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEHS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEHS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEHS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEHS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEHS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEHS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEHS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEHS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEHS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEHS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEHS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEHS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEHS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEHS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEHS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEHS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEHS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEHS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEHS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEHS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEHS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEHS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEHS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEHS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEHS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEHS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEHS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEHS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SEHU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SEHU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEWS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEWS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SEWS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SEWS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEWS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEWS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEWS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEWS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEWS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEWS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEWS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEWS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEWS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEWS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEWS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEWS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEWS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEWS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEWS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEWS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEWS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEWS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEWS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEWS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEWS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEWS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEWS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEWS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEWS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEWS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEWS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEWS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEWS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEWS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEWS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEWS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEWS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEWS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEWS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_2D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_2D( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_2D( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_2D( SEWU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_2D( SEWU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_2D( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_2D( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_2D( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_2D( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_2D( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_2D( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_2D( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_2D( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_2D( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_2D( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_2D( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_2D( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_2D( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_2D( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_2D( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_2D( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_2D( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_2D( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_2D( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_2D( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_2D( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_2D( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_2D( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_2D( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_2D( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_2D( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_2D( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_2D( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_2D( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_2D( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_2D( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_2D( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_2D( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_2D( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_2D( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVBHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVBHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVBHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVBHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVBHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVBHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVBHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVBHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVBHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVBHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVBHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVBHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVBHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVBHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVBHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVBHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVBHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVBHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVBHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVBHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVBHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVBHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVBHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVBWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVBWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVBWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVBWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVBWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVBWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVBWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVBWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVBWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVBWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVBWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVBWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVBWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVBWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVBWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVBWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVBWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVBWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVBWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVBWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVBWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVBWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVBWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVHBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVHBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVHBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVHBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVHBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVHBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVHBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVHBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVHBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVHBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVHBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVHBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVHBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVHBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVHBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVHBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVHBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVHBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVHBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVHBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVHBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVHBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVHBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVHWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVHWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVHWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVHWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVHWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVHWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVHWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVHWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVHWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVHWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVHWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVHWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVHWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVHWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVHWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVHWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVHWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVHWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVHWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVHWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVHWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVHWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVHWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVWBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVWBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVWBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVWBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVWBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVWBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVWBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVWBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVWBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVWBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVWBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVWBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVWBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVWBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVWBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVWBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVWBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVWBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVWBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVWBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVWBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVWBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVWBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVWHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVWHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVWHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVWHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVWHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVWHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVWHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVWHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVWHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVWHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVWHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVWHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVWHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVWHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVWHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVWHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVWHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVWHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVWHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVWHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVWHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVWHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVWHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( VVWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( VVWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( SVBS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVBHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVBHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVBHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVBHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVBHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVBHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVBHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVBHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVBHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVBHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVBHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVBHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVBHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVBHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVBHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVBHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVBHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVBHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVBHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVBHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVBHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVBHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVBHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVBWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVBWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVBWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVBWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVBWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVBWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVBWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVBWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVBWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVBWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVBWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVBWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVBWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVBWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVBWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVBWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVBWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVBWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVBWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVBWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVBWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVBWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVBWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVHBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVHBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVHBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVHBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVHBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVHBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVHBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVHBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVHBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVHBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVHBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVHBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVHBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVHBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVHBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVHBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVHBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVHBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVHBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVHBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVHBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVHBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVHBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( SVHS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVHWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVHWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVHWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVHWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVHWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVHWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVHWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVHWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVHWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVHWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVHWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVHWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVHWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVHWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVHWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVHWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVHWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVHWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVHWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVHWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVHWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVHWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVHWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVWBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVWBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVWBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVWBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVWBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVWBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVWBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVWBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVWBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVWBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVWBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVWBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVWBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVWBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVWBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVWBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVWBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVWBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVWBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVWBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVWBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVWBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVWBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVWHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVWHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVWHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVWHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVWHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVWHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVWHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVWHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVWHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVWHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVWHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVWHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVWHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVWHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVWHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVWHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVWHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVWHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVWHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVWHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVWHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVWHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVWHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( SVWS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_3D( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_3D( SVWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_3D( SVWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_3D( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_3D( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_3D( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_3D( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_3D( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_3D( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_3D( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_3D( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_3D( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_3D( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_3D( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_3D( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_3D( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_3D( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_3D( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEBHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEBHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEBHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEBHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEBWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEBWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEBWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEBWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEHBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEHBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEHBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEHBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEHWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEHWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEHWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEHWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEWBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEWBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEWBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEWBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEWHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEWHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEWHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEWHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( VEWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( VEWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEBS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEBS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( SEBS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( SEBS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEBS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEBS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEBS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEBS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEBS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEBS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEBS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEBS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEBS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEBS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEBS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEBS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEBS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEBS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEBS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEBS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEBS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEBS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEBS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEBS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEBS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEBS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEBS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEBS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEBS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEBS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEBS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEBS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEBS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEBS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEBS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEBS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEBS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEBS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEBS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( SEBU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( SEBU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEHS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEHS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( SEHS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( SEHS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEHS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEHS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEHS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEHS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEHS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEHS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEHS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEHS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEHS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEHS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEHS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEHS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEHS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEHS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEHS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEHS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEHS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEHS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEHS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEHS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEHS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEHS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEHS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEHS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEHS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEHS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEHS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEHS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEHS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEHS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEHS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEHS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEHS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEHS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEHS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( SEHU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( SEHU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEWS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEWS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( SEWS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( SEWS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEWS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEWS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEWS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEWS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEWS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEWS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEWS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEWS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEWS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEWS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEWS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEWS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEWS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEWS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEWS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEWS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEWS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEWS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEWS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEWS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEWS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEWS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEWS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEWS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEWS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEWS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEWS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEWS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEWS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEWS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEWS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEWS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEWS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEWS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEWS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_3D( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_3D( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_3D( SEWU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_3D( SEWU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_3D( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_3D( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_3D( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_3D( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_3D( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_3D( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_3D( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_3D( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_3D( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_3D( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_3D( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_3D( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_3D( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_3D( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_3D( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_3D( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_3D( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_3D( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_3D( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_3D( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_3D( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_3D( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_3D( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_3D( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_3D( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_3D( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_3D( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_3D( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_3D( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_3D( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_3D( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_3D( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_3D( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_3D( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_3D( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVBHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVBHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVBHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVBHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVBHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVBHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVBHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVBHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVBHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVBHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVBHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVBHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVBHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVBHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVBHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVBHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVBHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVBHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVBWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVBWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVBWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVBWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVBWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVBWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVBWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVBWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVBWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVBWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVBWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVBWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVBWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVBWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVBWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVBWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVBWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVBWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVBWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVBWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVBWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVBWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVBWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVBWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVBWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVBWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVBWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVBWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVBWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVBWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVBWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVBWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVBWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVBWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVBWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVBWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVBWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVBWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVBWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVBWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVBWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVBWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVBWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVBWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVBWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVBWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVBWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVBWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVBWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVHBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVHBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVHBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVHBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVHBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVHBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVHBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVHBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVHBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVHBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVHBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVHBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVHBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVHBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVHBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVHBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVHBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVHBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVHWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVHWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVHWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVHWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVHWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVHWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVHWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVHWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVHWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVHWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVHWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVHWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVHWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVHWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVHWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVHWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVHWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVHWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVHWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVHWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVHWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVHWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVHWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVHWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVHWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVHWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVHWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVHWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVHWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVHWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVHWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVHWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVHWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVHWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVHWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVHWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVHWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVHWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVHWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVHWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVHWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVHWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVHWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVHWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVHWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVHWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVHWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVHWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVHWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWBS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWBS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVWBS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVWBS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWBS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWBS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWBS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWBS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWBS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWBS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWBS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWBS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWBS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWBS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWBS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWBS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWBS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVWBS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVWBS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVWBS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVWBS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVWBS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVWBS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWBS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWBS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWBS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWBS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWBS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWBS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWBS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWBS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWBS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWBS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWBS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWBS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWBS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWBS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWBS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWBS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVWBS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWBU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWBU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVWBU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVWBU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWBU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWBU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWBU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWBU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWBU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWBU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWBU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWBU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWBU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWBU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWBU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWBU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWBU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVWBU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVWBU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVWBU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVWBU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVWBU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVWBU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWBU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWBU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWBU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWBU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWBU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWBU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWBU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWBU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWBU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWBU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWBU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWBU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWBU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWBU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWBU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWBU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVWBU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWHS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWHS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVWHS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVWHS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWHS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWHS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWHS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWHS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWHS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWHS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWHS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWHS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWHS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWHS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWHS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWHS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWHS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVWHS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVWHS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVWHS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVWHS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVWHS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVWHS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWHS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWHS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWHS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWHS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWHS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWHS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWHS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWHS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWHS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWHS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWHS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWHS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWHS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWHS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWHS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWHS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVWHS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWHU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWHU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVWHU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVWHU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWHU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWHU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWHU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWHU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWHU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWHU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWHU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWHU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWHU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWHU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWHU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWHU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWHU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVWHU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVWHU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVWHU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVWHU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVWHU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVWHU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWHU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWHU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWHU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWHU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWHU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWHU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWHU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWHU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWHU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWHU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWHU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWHU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWHU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWHU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWHU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWHU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVWHU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWS, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWS, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVWS, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVWS, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWS, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWS, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWS, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWS, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWS, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWS, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWS, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWS, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWS, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWS, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWS, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWS, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWS, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVWS, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVWS, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVWS, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVWS, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVWS, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVWS, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWS, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWS, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWS, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWS, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWS, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWS, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWS, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWS, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWS, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWS, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWS, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWS, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWS, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWS, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWS, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWS, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVWS, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VVWU, VADD, v_out, v_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( VVWU, VSUB, v_out, v_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VVWU, VADDFXP, v_out, v_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VVWU, VSUBFXP, v_out, v_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( VVWU, VADDC, v_out, v_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VVWU, VSUBB, v_out, v_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VVWU, VABSDIFF, v_out, v_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( VVWU, VMUL, v_out, v_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VVWU, VMULHI, v_out, v_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VVWU, VMULFXP, v_out, v_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( VVWU, VAND, v_out, v_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( VVWU, VOR, v_out, v_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( VVWU, VXOR, v_out, v_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( VVWU, VSHL, v_out, v_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( VVWU, VSHR, v_out, v_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( VVWU, VROTL, v_out, v_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( VVWU, VROTR, v_out, v_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VVWU, VCMV_LEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VVWU, VCMV_GTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VVWU, VCMV_LTZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VVWU, VCMV_GEZ, v_out, v_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VVWU, VCMV_Z, v_out, v_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VVWU, VCMV_NZ, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VVWU, VCUSTOM0, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VVWU, VCUSTOM1, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VVWU, VCUSTOM2, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VVWU, VCUSTOM3, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VVWU, VCUSTOM4, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VVWU, VCUSTOM5, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VVWU, VCUSTOM6, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VVWU, VCUSTOM7, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VVWU, VCUSTOM8, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VVWU, VCUSTOM9, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VVWU, VCUSTOM10, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VVWU, VCUSTOM11, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VVWU, VCUSTOM12, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VVWU, VCUSTOM13, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VVWU, VCUSTOM14, v_out, v_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VVWU, VCUSTOM15, v_out, v_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( VVWU, VMOV, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( SVBS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( SVBU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVBHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVBHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVBHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVBHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVBHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVBHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVBHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVBHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVBHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVBHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVBHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVBHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVBHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVBHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVBHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVBHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVBHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVBHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVBHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVBHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_byte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVBWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVBWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVBWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVBWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVBWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVBWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVBWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVBWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVBWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVBWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_ubyte_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVBWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVBWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVBWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVBWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVBWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVBWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVBWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVBWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVBWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVBWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVBWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVBWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVBWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVBWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVBWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVBWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVBWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVBWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVBWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVBWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVBWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVBWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVBWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVBWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVBWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVBWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVBWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVBWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVBWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVBWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVBWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVBWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVBWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVBWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVBWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVBWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVBWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVBWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVBWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVHBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVHBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVHBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVHBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVHBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVHBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVHBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVHBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVHBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVHBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVHBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVHBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVHBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVHBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVHBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVHBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVHBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVHBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVHBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVHBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( SVHS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( SVHU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_half_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVHWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVHWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVHWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVHWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVHWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVHWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVHWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVHWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVHWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVHWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uhalf_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVHWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVHWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVHWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVHWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVHWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVHWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVHWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVHWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVHWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVHWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVHWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVHWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVHWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVHWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVHWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVHWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVHWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVHWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVHWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVHWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVHWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVHWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVHWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVHWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVHWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVHWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVHWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVHWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVHWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVHWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVHWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVHWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVHWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVHWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVHWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVHWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVHWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVHWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVHWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWBS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWBS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVWBS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVWBS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWBS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWBS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWBS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWBS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWBS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWBS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVWBS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVWBS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWBS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWBS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWBS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWBS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWBS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVWBS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVWBS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVWBS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVWBS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVWBS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVWBS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWBS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWBS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWBS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWBS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWBS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWBS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWBS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWBS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWBS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWBS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWBS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWBS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWBS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWBS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWBS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWBS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWBU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWBU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVWBU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVWBU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWBU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWBU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWBU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWBU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWBU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWBU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVWBU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVWBU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWBU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWBU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWBU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWBU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWBU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVWBU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVWBU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVWBU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVWBU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVWBU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVWBU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWBU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWBU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWBU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWBU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWBU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWBU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWBU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWBU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWBU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWBU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWBU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWBU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWBU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWBU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWBU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWBU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWHS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWHS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVWHS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVWHS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWHS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWHS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWHS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWHS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWHS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWHS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVWHS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVWHS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWHS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWHS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWHS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWHS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWHS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVWHS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVWHS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVWHS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVWHS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVWHS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVWHS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWHS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWHS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWHS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWHS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWHS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWHS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWHS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWHS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWHS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWHS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWHS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWHS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWHS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWHS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWHS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWHS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWHU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWHU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVWHU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVWHU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWHU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWHU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWHU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWHU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWHU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWHU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVWHU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVWHU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWHU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWHU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWHU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWHU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWHU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVWHU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVWHU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVWHU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVWHU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVWHU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVWHU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWHU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWHU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWHU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWHU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWHU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWHU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWHU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWHU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWHU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWHU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWHU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWHU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWHU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWHU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWHU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWHU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_word_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWS, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWS, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVWS, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVWS, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWS, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWS, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWS, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWS, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWS, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWS, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVWS, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVWS, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWS, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWS, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWS, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWS, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWS, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVWS, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVWS, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVWS, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVWS, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVWS, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVWS, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWS, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWS, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWS, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWS, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWS, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWS, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWS, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWS, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWS, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWS, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWS, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWS, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWS, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWS, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWS, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWS, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( SVWS, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_uword_t *v_in2 ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SVWU, VADD, v_out, s_in1, v_in2 );
		break;
	case VSUB:
		vbxasm_acc_3D( SVWU, VSUB, v_out, s_in1, v_in2 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SVWU, VADDFXP, v_out, s_in1, v_in2 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SVWU, VSUBFXP, v_out, s_in1, v_in2 );
		break;
	case VADDC:
		vbxasm_acc_3D( SVWU, VADDC, v_out, s_in1, v_in2 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SVWU, VSUBB, v_out, s_in1, v_in2 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SVWU, VABSDIFF, v_out, s_in1, v_in2 );
		break;
	case VMUL:
		vbxasm_acc_3D( SVWU, VMUL, v_out, s_in1, v_in2 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SVWU, VMULHI, v_out, s_in1, v_in2 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SVWU, VMULFXP, v_out, s_in1, v_in2 );
		break;
	case VAND:
		vbxasm_acc_3D( SVWU, VAND, v_out, s_in1, v_in2 );
		break;
	case VOR:
		vbxasm_acc_3D( SVWU, VOR, v_out, s_in1, v_in2 );
		break;
	case VXOR:
		vbxasm_acc_3D( SVWU, VXOR, v_out, s_in1, v_in2 );
		break;
	case VSHL:
		vbxasm_acc_3D( SVWU, VSHL, v_out, s_in1, v_in2 );
		break;
	case VSHR:
		vbxasm_acc_3D( SVWU, VSHR, v_out, s_in1, v_in2 );
		break;
	case VROTL:
		vbxasm_acc_3D( SVWU, VROTL, v_out, s_in1, v_in2 );
		break;
	case VROTR:
		vbxasm_acc_3D( SVWU, VROTR, v_out, s_in1, v_in2 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SVWU, VCMV_LEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SVWU, VCMV_GTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SVWU, VCMV_LTZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SVWU, VCMV_GEZ, v_out, s_in1, v_in2 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SVWU, VCMV_Z, v_out, s_in1, v_in2 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SVWU, VCMV_NZ, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SVWU, VCUSTOM0, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SVWU, VCUSTOM1, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SVWU, VCUSTOM2, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SVWU, VCUSTOM3, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SVWU, VCUSTOM4, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SVWU, VCUSTOM5, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SVWU, VCUSTOM6, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SVWU, VCUSTOM7, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SVWU, VCUSTOM8, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SVWU, VCUSTOM9, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SVWU, VCUSTOM10, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SVWU, VCUSTOM11, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SVWU, VCUSTOM12, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SVWU, VCUSTOM13, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SVWU, VCUSTOM14, v_out, s_in1, v_in2 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SVWU, VCUSTOM15, v_out, s_in1, v_in2 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1 )
{
	switch(v_op)
	{
	case VMOV:
		vbxasm_acc_3D( SVWU, VMOV, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEBS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEBS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEBS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEBS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEBS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEBS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEBU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEBU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEBU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEBU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEBU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEBU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEBHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEBHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEBHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEBHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_byte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEBWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEBWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_ubyte_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEBWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEBWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEBWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEBWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEBWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEBWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEBWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEBWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEBWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEBWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEBWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEBWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEBWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEBWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEBWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEBWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEBWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEBWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEBWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEBWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEBWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEBWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEBWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEBWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEBWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEBWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEBWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEBWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEBWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEBWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEBWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEBWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEBWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEHBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEHBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEHBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEHBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEHS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEHS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEHS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEHS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEHS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEHS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEHU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEHU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEHU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEHU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEHU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEHU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_half_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEHWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEHWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uhalf_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEHWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEHWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEHWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEHWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEHWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEHWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEHWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEHWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEHWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEHWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEHWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEHWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEHWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEHWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEHWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEHWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEHWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEHWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEHWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEHWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEHWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEHWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEHWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEHWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEHWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEHWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEHWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEHWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEHWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEHWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEHWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEHWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEHWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWBS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWBS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEWBS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEWBS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWBS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWBS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWBS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWBS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWBS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWBS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWBS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWBS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWBS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWBS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWBS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWBS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWBS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWBS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWBS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWBS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWBS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWBS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWBS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWBS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWBS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWBS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWBS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWBS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWBS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWBS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWBS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWBS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWBS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWBU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWBU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEWBU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEWBU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWBU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWBU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWBU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWBU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWBU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWBU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWBU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWBU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWBU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWBU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWBU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWBU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWBU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWBU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWBU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWBU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWBU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWBU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWBU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWBU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWBU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWBU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWBU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWBU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWBU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWBU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWBU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWBU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWBU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWHS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWHS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEWHS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEWHS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWHS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWHS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWHS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWHS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWHS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWHS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWHS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWHS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWHS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWHS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWHS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWHS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWHS, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWHS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWHS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWHS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWHS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWHS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWHS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWHS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWHS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWHS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWHS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWHS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWHS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWHS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWHS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWHS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWHS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWHU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWHU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEWHU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEWHU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWHU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWHU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWHU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWHU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWHU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWHU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWHU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWHU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWHU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWHU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWHU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWHU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWHU, VROTR, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWHU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWHU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWHU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWHU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWHU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWHU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWHU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWHU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWHU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWHU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWHU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWHU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWHU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWHU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWHU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWHU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWS, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWS, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEWS, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEWS, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWS, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWS, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWS, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWS, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWS, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWS, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWS, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWS, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWS, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWS, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWS, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWS, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWS, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEWS, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEWS, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEWS, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEWS, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEWS, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEWS, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWS, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWS, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWS, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWS, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWS, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWS, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWS, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWS, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWS, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWS, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWS, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWS, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWS, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWS, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWS, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWS, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t *v_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( VEWU, VADD, v_out, v_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( VEWU, VSUB, v_out, v_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( VEWU, VADDFXP, v_out, v_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( VEWU, VSUBFXP, v_out, v_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( VEWU, VADDC, v_out, v_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( VEWU, VSUBB, v_out, v_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( VEWU, VABSDIFF, v_out, v_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( VEWU, VMUL, v_out, v_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( VEWU, VMULHI, v_out, v_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( VEWU, VMULFXP, v_out, v_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( VEWU, VAND, v_out, v_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( VEWU, VOR, v_out, v_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( VEWU, VXOR, v_out, v_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( VEWU, VSHL, v_out, v_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( VEWU, VSHR, v_out, v_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( VEWU, VROTL, v_out, v_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( VEWU, VROTR, v_out, v_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( VEWU, VCMV_LEZ, v_out, v_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( VEWU, VCMV_GTZ, v_out, v_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( VEWU, VCMV_LTZ, v_out, v_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( VEWU, VCMV_GEZ, v_out, v_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( VEWU, VCMV_Z, v_out, v_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( VEWU, VCMV_NZ, v_out, v_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( VEWU, VCUSTOM0, v_out, v_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( VEWU, VCUSTOM1, v_out, v_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( VEWU, VCUSTOM2, v_out, v_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( VEWU, VCUSTOM3, v_out, v_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( VEWU, VCUSTOM4, v_out, v_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( VEWU, VCUSTOM5, v_out, v_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( VEWU, VCUSTOM6, v_out, v_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( VEWU, VCUSTOM7, v_out, v_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( VEWU, VCUSTOM8, v_out, v_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( VEWU, VCUSTOM9, v_out, v_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( VEWU, VCUSTOM10, v_out, v_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( VEWU, VCUSTOM11, v_out, v_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( VEWU, VCUSTOM12, v_out, v_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( VEWU, VCUSTOM13, v_out, v_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( VEWU, VCUSTOM14, v_out, v_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( VEWU, VCUSTOM15, v_out, v_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_enum_t *v_enum )
{
	switch(v_op)
	{
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_byte_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEBS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEBS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SEBS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SEBS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEBS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEBS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEBS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEBS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEBS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEBS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEBS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEBS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEBS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEBS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEBS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEBS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEBS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEBS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEBS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEBS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEBS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEBS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEBS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEBS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEBS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEBS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEBS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEBS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEBS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEBS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEBS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEBS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEBS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEBS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEBS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEBS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEBS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEBS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEBS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_ubyte_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEBU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEBU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SEBU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SEBU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEBU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEBU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEBU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEBU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEBU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEBU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEBU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEBU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEBU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEBU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEBU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEBU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEBU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEBU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEBU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEBU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEBU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEBU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEBU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEBU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEBU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEBU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEBU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEBU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEBU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEBU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEBU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEBU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEBU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEBU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEBU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEBU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEBU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEBU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEBU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_half_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEHS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEHS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SEHS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SEHS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEHS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEHS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEHS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEHS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEHS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEHS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEHS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEHS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEHS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEHS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEHS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEHS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEHS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEHS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEHS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEHS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEHS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEHS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEHS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEHS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEHS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEHS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEHS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEHS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEHS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEHS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEHS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEHS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEHS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEHS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEHS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEHS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEHS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEHS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEHS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uhalf_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEHU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEHU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SEHU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SEHU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEHU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEHU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEHU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEHU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEHU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEHU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEHU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEHU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEHU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEHU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEHU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEHU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEHU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEHU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEHU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEHU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEHU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEHU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEHU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEHU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEHU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEHU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEHU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEHU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEHU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEHU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEHU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEHU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEHU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEHU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEHU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEHU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEHU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEHU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEHU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_word_t *v_out, vbx_word_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEWS, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEWS, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SEWS, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SEWS, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEWS, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEWS, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEWS, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEWS, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEWS, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEWS, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEWS, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEWS, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEWS, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEWS, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEWS, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEWS, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEWS, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEWS, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEWS, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEWS, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEWS, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEWS, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEWS, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEWS, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEWS, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEWS, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEWS, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEWS, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEWS, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEWS, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEWS, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEWS, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEWS, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEWS, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEWS, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEWS, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEWS, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEWS, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEWS, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}

__attribute__((always_inline)) static inline void vbxx_acc_3D( vinstr_t v_op, vbx_uword_t *v_out, vbx_uword_t s_in1, vbx_enum_t *v_enum ){
	switch(v_op)
	{
	case VADD:
		vbxasm_acc_3D( SEWU, VADD, v_out, s_in1, 0 );
		break;
	case VSUB:
		vbxasm_acc_3D( SEWU, VSUB, v_out, s_in1, 0 );
		break;
	case VADDFXP:
		vbxasm_acc_3D( SEWU, VADDFXP, v_out, s_in1, 0 );
		break;
	case VSUBFXP:
		vbxasm_acc_3D( SEWU, VSUBFXP, v_out, s_in1, 0 );
		break;
	case VADDC:
		vbxasm_acc_3D( SEWU, VADDC, v_out, s_in1, 0 );
		break;
	case VSUBB:
		vbxasm_acc_3D( SEWU, VSUBB, v_out, s_in1, 0 );
		break;
	case VABSDIFF:
		vbxasm_acc_3D( SEWU, VABSDIFF, v_out, s_in1, 0 );
		break;
	case VMUL:
		vbxasm_acc_3D( SEWU, VMUL, v_out, s_in1, 0 );
		break;
	case VMULHI:
		vbxasm_acc_3D( SEWU, VMULHI, v_out, s_in1, 0 );
		break;
	case VMULFXP:
		vbxasm_acc_3D( SEWU, VMULFXP, v_out, s_in1, 0 );
		break;
	case VAND:
		vbxasm_acc_3D( SEWU, VAND, v_out, s_in1, 0 );
		break;
	case VOR:
		vbxasm_acc_3D( SEWU, VOR, v_out, s_in1, 0 );
		break;
	case VXOR:
		vbxasm_acc_3D( SEWU, VXOR, v_out, s_in1, 0 );
		break;
	case VSHL:
		vbxasm_acc_3D( SEWU, VSHL, v_out, s_in1, 0 );
		break;
	case VSHR:
		vbxasm_acc_3D( SEWU, VSHR, v_out, s_in1, 0 );
		break;
	case VROTL:
		vbxasm_acc_3D( SEWU, VROTL, v_out, s_in1, 0 );
		break;
	case VROTR:
		vbxasm_acc_3D( SEWU, VROTR, v_out, s_in1, 0 );
		break;
	case VCMV_LEZ:
		vbxasm_acc_3D( SEWU, VCMV_LEZ, v_out, s_in1, 0 );
		break;
	case VCMV_GTZ:
		vbxasm_acc_3D( SEWU, VCMV_GTZ, v_out, s_in1, 0 );
		break;
	case VCMV_LTZ:
		vbxasm_acc_3D( SEWU, VCMV_LTZ, v_out, s_in1, 0 );
		break;
	case VCMV_GEZ:
		vbxasm_acc_3D( SEWU, VCMV_GEZ, v_out, s_in1, 0 );
		break;
	case VCMV_Z:
		vbxasm_acc_3D( SEWU, VCMV_Z, v_out, s_in1, 0 );
		break;
	case VCMV_NZ:
		vbxasm_acc_3D( SEWU, VCMV_NZ, v_out, s_in1, 0 );
		break;
	case VCUSTOM0:
		vbxasm_acc_3D( SEWU, VCUSTOM0, v_out, s_in1, 0 );
		break;
	case VCUSTOM1:
		vbxasm_acc_3D( SEWU, VCUSTOM1, v_out, s_in1, 0 );
		break;
	case VCUSTOM2:
		vbxasm_acc_3D( SEWU, VCUSTOM2, v_out, s_in1, 0 );
		break;
	case VCUSTOM3:
		vbxasm_acc_3D( SEWU, VCUSTOM3, v_out, s_in1, 0 );
		break;
	case VCUSTOM4:
		vbxasm_acc_3D( SEWU, VCUSTOM4, v_out, s_in1, 0 );
		break;
	case VCUSTOM5:
		vbxasm_acc_3D( SEWU, VCUSTOM5, v_out, s_in1, 0 );
		break;
	case VCUSTOM6:
		vbxasm_acc_3D( SEWU, VCUSTOM6, v_out, s_in1, 0 );
		break;
	case VCUSTOM7:
		vbxasm_acc_3D( SEWU, VCUSTOM7, v_out, s_in1, 0 );
		break;
	case VCUSTOM8:
		vbxasm_acc_3D( SEWU, VCUSTOM8, v_out, s_in1, 0 );
		break;
	case VCUSTOM9:
		vbxasm_acc_3D( SEWU, VCUSTOM9, v_out, s_in1, 0 );
		break;
	case VCUSTOM10:
		vbxasm_acc_3D( SEWU, VCUSTOM10, v_out, s_in1, 0 );
		break;
	case VCUSTOM11:
		vbxasm_acc_3D( SEWU, VCUSTOM11, v_out, s_in1, 0 );
		break;
	case VCUSTOM12:
		vbxasm_acc_3D( SEWU, VCUSTOM12, v_out, s_in1, 0 );
		break;
	case VCUSTOM13:
		vbxasm_acc_3D( SEWU, VCUSTOM13, v_out, s_in1, 0 );
		break;
	case VCUSTOM14:
		vbxasm_acc_3D( SEWU, VCUSTOM14, v_out, s_in1, 0 );
		break;
	case VCUSTOM15:
		vbxasm_acc_3D( SEWU, VCUSTOM15, v_out, s_in1, 0 );
		break;
	default:
		break;
	}
}


template<class T>
void vbx_setup_mask_masked(vinstr_t vinstr,T* src)
{
	switch(vinstr){
	case VCMV_LEZ:
		vbx_setup_mask_masked_(VCMV_LEZ,src);
		break;
	case VCMV_GTZ:
		vbx_setup_mask_masked_(VCMV_GTZ,src);
		break;
	case VCMV_LTZ:
		vbx_setup_mask_masked_(VCMV_LTZ,src);
		break;
	case VCMV_GEZ:
		vbx_setup_mask_masked_(VCMV_GEZ,src);
		break;
	case VCMV_Z:
		vbx_setup_mask_masked_(VCMV_Z,src);
		break;
	case VCMV_NZ:
		vbx_setup_mask_masked_(VCMV_NZ,src);
		break;
	default:break;
	}

}
template<class T>
void vbx_setup_mask(vinstr_t vinstr,T* src)
{
	switch(vinstr){
	case VCMV_LEZ:
		vbx_setup_mask_(VCMV_LEZ,src);
		break;
	case VCMV_GTZ:
		vbx_setup_mask_(VCMV_GTZ,src);
		break;
	case VCMV_LTZ:
		vbx_setup_mask_(VCMV_LTZ,src);
		break;
	case VCMV_GEZ:
		vbx_setup_mask_(VCMV_GEZ,src);
		break;
	case VCMV_Z:
		vbx_setup_mask_(VCMV_Z,src);
		break;
	case VCMV_NZ:
		vbx_setup_mask_(VCMV_NZ,src);
		break;
	default:break;
	}

}

#endif // __VBXX_HPP
